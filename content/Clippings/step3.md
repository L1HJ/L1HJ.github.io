## 在大语言模型推理中克服非确定性

霍勒斯·何与思考机器公司的其他人合作

2025年9月10日

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/cover.svg)

可重复性是科学进步的基石。然而，从大型语言模型中获得可重复的结果非常困难。

例如，你可能会观察到多次向ChatGPT询问同一个问题会得到不同的结果。这本身并不令人惊讶，因为从语言模型中获得结果涉及“采样”，这个过程将语言模型的输出转换为概率分布并随机选择一个标记。

更令人惊讶的是，即使我们将温度调低到0（这意味着大语言模型总是选择概率最高的标记，称为贪心采样，从而使采样在理论上具有确定性），大语言模型API在实践中仍然**不**具有确定性（见过去在这里、这里或这里的讨论）。即使使用vLLM或SGLang等开源推理库在自己的硬件上运行推理，采样仍然不确定（见这里或这里）。

但为什么大语言模型推理引擎**不**具有确定性？一个常见的假设是，浮点非结合性和并发执行的某种组合会导致非确定性，这取决于哪个并发核心首先完成。我们将此称为大语言模型推理非确定性的“并发+浮点”假设。例如，最近的一篇 arXiv 预印本写道：

> GPU中的浮点运算表现出非结合性，意味着__MATH_FORMULA_0__由于有限精度和舍入误差。这一特性直接影响了Transformer架构中注意力分数和对数几率的计算，其中多个线程的并行操作可能会根据执行顺序产生不同的结果。

你还可以在其他地方找到“并发+浮点”假设的重复，比如这里（*“存在速度权衡，为了使端点快速使用了GPU，GPU进行并行[非确定性]计算。任何现代GPU神经网络计算都将受到这些因素的影响。”*），或这里（*“由于GPU高度并行化，每次执行时加法或乘法的顺序可能不同，这可能会级联导致输出的微小差异。”*）。

虽然这个假设并非完全错误，但它没有揭示全貌。例如，即使在GPU上，对相同数据反复运行相同的矩阵乘法总是会提供按位相等的结果。我们肯定在使用浮点数。而且我们的GPU肯定有很多并发。为什么在这个测试中我们没有看到非确定性？

```
A = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
B = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
ref = torch.mm(A, B)
for _ in range(1000):
    assert (torch.mm(A, B) - ref).abs().max().item() == 0
```
要理解大语言模型推理非确定性的真正原因，我们必须深入探究。

不幸的是，即使*定义*大语言模型推理具有确定性意味着什么都很困难。也许令人困惑的是，以下陈述同时成立：

1. GPU上的一些内核是**非确定性**的。
2. 然而，语言模型前向传播中使用的所有内核都是**确定性**的。
3. 此外，大语言模型推理服务器（如vLLM）的前向传播也可以被声称是**确定性**的。
4. 然而，从任何使用推理服务器的人的角度来看，结果是**非确定性**的。

在这篇文章中，我们将解释为什么“并发+浮点”假设没有抓住要点，揭示大语言模型推理非确定性背后的真正罪魁祸首，并解释如何克服非确定性，在大语言模型推理中获得真正可重复的结果。

### 原罪：浮点非结合性

在谈论非确定性之前，解释为什么会有数值差异是有用的。毕竟，我们通常将机器学习模型视为遵循交换性或结合性等结构规则的数学函数。我们的机器学习库不应该为我们提供一个“数学上正确”的结果吗？

罪魁祸首是**浮点非结合性**。也就是说，对于浮点数：

$$ (a + b) + c \neq a + (b + c) $$
```
(0.1 + 1e20) - 1e20
>>> 0
0.1 + (1e20 - 1e20)
>>> 0.1
```
具有讽刺意味的是，打破结合性正是浮点数有用的原因。

浮点数有用是因为它们允许“动态”精度水平。为了解释的目的，我们将使用基数10（而不是二进制），其中浮点数的格式为$\text{mantissa} \* 10^\text{exponent}$我们还将使用3位尾数和1位指数。
例如，对于值3450，我们可以精确表示为$3.45 \* 10^3$. We can also represent much smaller values like 0.486 as$4.86 \* 10^{-1}$通过这种方式，浮点允许我们表示非常小和非常大的值。在科学领域，我们可能会说浮点允许我们保持恒定数量的“有效数字”。
如果你将两个指数相同的浮点数相加，看起来类似于整数加法。例如，123（$1.23 \* 10^2$) + 456 ($4.56 \* 10^2$) results in 579 ($5.79 \* 10^2$
但当我们将两个指数不同的浮点数相加时，比如1230和23.4，会发生什么呢？在这种情况下，精确结果是1253.4。然而，我们一次只能保持3位精度。因此，浮点加法会*舍弃*最后2位并得到值$1.25 \* 10^3$或1250）。
1.23 × 10²

+

3.45 × 10¹

=

1.575 × 10²

精确值：1575

我们需要3位精度来表示1230，需要3位精度来表示23.4。然而，将这两个数相加得到的数需要5位精度来表示（1253.4）。然后我们的浮点格式必须舍弃末尾的34。从某种意义上说，我们实际上在相加之前将原来的23.4四舍五入到了20.0。

然而，在这一点上，我们已经破坏了信息。请注意，每次我们将两个具有不同“量级”（即不同指数）的浮点数相加时都会发生这种情况。而且，将具有不同指数的浮点数相加总是会发生。事实上，如果我们能保证永远不需要不同的指数，我们就可以只用整数！

换句话说，每次我们以不同的顺序相加浮点数，都可能得到完全不同的结果。举一个极端的例子，根据顺序，这个数组求和可能有102种不同的结果。

```
import random

vals = [1e-10, 1e-5, 1e-2, 1]
vals = vals + [-v for v in vals]

results = []
random.seed(42)
for _ in range(10000):
    random.shuffle(vals)
    results.append(sum(vals))

results = sorted(set(results))
print(f"There are {len(results)} unique results: {results}")

## Output:

## There are 102 unique results: [-8.326672684688674e-17, -7.45931094670027e-17, ..., 8.326672684688674e-17]

```
虽然这是输出不一致的根本原因，但它没有直接回答非确定性来自哪里。它不能帮助我们理解为什么浮点数会以不同的顺序相加，这种情况何时发生，以及如何避免。

答案在于内核的实现方式。

### 为什么内核不总是以相同的顺序相加数字？

如上所述，内核以不同顺序相加数字的一个常见解释是“并发+浮点”假设。该假设指出，如果并发线程完成的顺序是不确定的，并且累加顺序取决于并发线程完成的顺序（例如使用原子加法），那么我们的累加顺序也将是不确定的。

令人困惑的是，尽管这可能导致不确定的内核，但并发（和原子加法）最终与大语言模型（LLM）推理的不确定性完全无关！为了解释真正的罪魁祸首是什么，让我们首先了解为什么现代GPU内核很少需要原子加法。

### 何时需要原子加法？

通常，GPU在许多“核心”（即SM）上并发启动程序。由于核心之间没有内在的同步，因此如果核心需要相互通信，这就会带来挑战。例如，如果所有核心都必须累积到同一个元素，您可以使用“原子加法”（有时称为“获取并添加”）。原子加法是“不确定的”——结果累积的顺序完全取决于哪个核心首先完成。

具体来说，想象一下您正在用100个核心减少一个100元素的向量（例如`torch.sum()`）。尽管您可以并行加载所有100个元素，但我们最终必须减少到一个元素。实现此目的的一种方法是使用某种“原子加法”原语，其中硬件保证所有加法都将被处理，但不保证顺序。

原子加法可确保每个核心的贡献都将反映在最终总和中。然而，它不保证贡献被添加的顺序。顺序完全取决于哪个核心首先完成，这是一个不确定的属性。因此，多次执行相同的并行程序可能会导致不确定的输出。

这通常就是人们所说的“不确定性”——您用完全相同的输入两次执行同一个内核，会得到不同的结果。这称为“运行间不确定性”，即您两次运行完全相同依赖项的同一个Python脚本，但得到不同的结果。

尽管并发原子加法确实会使内核不确定，但“对于绝大多数内核来说，原子加法并不是必需的”。事实上，在大语言模型的典型前向传播中，通常根本没有单个原子加法存在。

考虑到并行化约简可以受益于原子加法，这可能令人惊讶。原子加法不需要的主要原因有两个。

1. 通常在“批次”维度上有足够的并行性，因此我们不需要在约简维度上并行化。例如，假设我们不是约简单个100维向量，而是并行约简500个向量。在这种情况下，我们可以在每个核心中约简整个向量，并允许每个核心操作不同的向量。
2. 随着时间的推移，大多数神经网络库已经采用了各种在不牺牲性能的情况下实现确定性的策略。例如，我们可以执行“拆分”（或树）约简，将100元素的约简拆分为五个20元素的约简（从而实现五向并行性）。然后，为了合并剩下的五个元素，我们可以执行单独的“清理”约简（它不并行，但操作的元素足够少，成本低）或利用信号量（它确保每个并发线程块以确定性顺序累积）。信号量策略可以在此处找到描述。

由于这两个因素，对于绝大多数神经网络操作来说，避免原子加法是一个可忽略的性能损失。

仍然有一些常见操作在避免原子加法时会有显著的性能损失。例如，PyTorch中的`scatter_add`（`a[b] += c`）。然而，大语言模型中唯一常用的是FlashAttention反向传播。有趣的事实：您知道吗？广泛使用的Triton实现的FlashAttention反向传播在算法上与Tri Dao的FlashAttention-2论文不同？标准Triton实现会在反向传播中进行额外的重新计算，避免了原子加法，但代价是多消耗40%的FLOPs！

然而，大语言模型的前向传播涉及“没有需要原子加法的操作”。因此，大语言模型中的前向传播实际上是“运行间确定性的”。

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/deterministic.svg)

从推理服务器的角度来看，它“是”确定性的。给定完全相同的用户请求，它将始终提供相同的确定性输出。

维基百科写道：“确定性算法是给定特定输入时，始终产生相同输出的算法。”在这种情况下，给定完全相同的输入（即推理服务器正在处理的确切请求），前向传播始终产生完全相同的输出。

然而，前向传播本身是“确定性的”并不足以确保包含它的系统是确定性的。例如，如果我们请求的输出取决于并行的用户请求（例如批归一化）怎么办？由于每个单独的请求无法知道并行请求会是什么，从它们的角度来看，我们的整个大语言模型推理也是不确定的！

事实证明，我们请求的输出“确实”取决于并行的用户请求。不是因为我们以某种方式跨批次泄露信息——而是因为我们的前向传播缺乏“批次不变性”，导致我们请求的输出取决于我们前向传播的**批次大小**。

#### 批次不变性和“确定性”

为了解释批次不变性，让我们简化系统，仅看矩阵乘法。您可以假设所有矩阵乘法实现都是“运行间确定性的”。这并不完全正确，但大多数常见的矩阵乘法实现都有此属性。然而，它们不是“批次不变的”。换句话说，当批次大小改变时，批次中的每个元素可能会得到不同的结果。

这从数学角度来看是一个相当不寻常的属性。矩阵乘法应该在批次中的每个元素上都是“独立的”——批次中的其他元素或批次的大小都不应影响特定元素的计算结果。

然而，正如我们可以凭经验观察到的，这不是真的。

```
import torch
torch.set_default_device('cuda')

B = 2048
D = 4096
a = torch.linspace(-1000, 1000, B*D).reshape(B, D)
b = torch.linspace(-1000, 1000, D*D).reshape(D, D)

## Doing a matrix vector multiplication by taking

## the first element of the batch

out1 = torch.mm(a[:1], b)

## Doing a matrix matrix multiplication and then taking

## the first element of the batch

out2 = torch.mm(a, b)[:1]
print((out1 - out2).abs().max()) # tensor(1669.2500, device='cuda:0')
```
请注意，这**是**“运行间确定性的”。如果您多次运行脚本，它将确定性地返回相同的结果。它不是“硬件/软件版本不变的”——您的GPU/PyTorch版本可能返回不同的值，但它应该确定性地返回相同的值。

然而，当非批次不变的内核用作更大推理系统的一部分时，系统可能变得不确定。当您向推理端点发出查询时，从用户的角度来看，服务器承受的负载实际上是“不确定的”。负载决定了内核运行时的批次大小，从而改变了每个单独请求的最终结果！

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/nondeterministic.svg)

尽管推理服务器本身可以被声称是“确定性的”，但对于单个用户来说情况不同。从单个用户的角度来看，其他并发用户不是系统的“输入”，而是系统的一个不确定属性。这使得从每个用户的角度来看，大语言模型推理是“不确定的”。

如果你将内核不具有不变性的某个属性（即批量大小）与该属性的不确定性（即服务器所承受的负载）组合在一起，就会得到一个不确定的系统。

换句话说，**几乎所有大语言模型（LLM）推理端点都是不确定的主要原因是负载（从而批量大小）不确定地变化！** 这种不确定性并非GPU独有——从CPU或TPU提供服务的LLM推理端点也会有这种不确定性来源。

因此，如果我们想在推理服务器中避免不确定性，我们必须在我们的内核中实现批量不变性。为了理解如何实现这一点，让我们首先看看内核最初为什么不具有批量不变性。

### 我们如何使内核具有批量不变性？

为了使Transformer实现具有批量不变性，我们必须使每个内核都具有批量不变性。幸运的是，我们可以假设每个逐点操作都是批量不变的。尽管在诸如PyTorch的所有内核中这都是事实，但这并非固有如此。例如，CPU上的一些内核实现会在数组的某些部分使用向量化内在函数，而在其他部分使用非向量化内在函数，并且这些内在函数不一定总是具有按位相同的数值。因此，我们只需要担心涉及归约的3种操作——均方根归一化（RMSNorm）、矩阵乘法和注意力机制。与并行性相关的归约不在本次讨论范围内，但同样的原理适用。一个可能有用的小知识是，带有CUDA 12.8+的Blackwell和Hopper上，NVLink-Sharp交换内归约是确定性的。与许多事情一样，此信息可以在NCCL的GitHub问题中找到

方便的是，这些（操作）也按难度递增的顺序排列。每一个都需要一些额外的考虑来实现具有合理性能的批量不变性。让我们首先讨论均方根归一化（RMSNorm）。

#### 批量不变的均方根归一化（RMSNorm）

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/rmsnorm-01.svg)

数据并行均方根归一化（RMSNorm）** 理想情况下，我们希望在并行化策略中避免核心之间的通信。实现这一点的一种方法是为每个核心分配一个批量元素，从而确保每个归约完全在单个核心内完成。这就是所谓的“数据并行”策略，因为我们只是沿着不需要通信的维度进行并行化。在此示例中，我们有四行和四个核心，使核心饱和。

均方根归一化（RMSNorm）可以实现为：

```

## x: [batch_size, hidden_dim]

## weight: [hidden_dim]

def rms_norm(x, weight):
    return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight
```
批量不变性的要求是，**每个元素的归约顺序必须固定，无论内核的批量大小如何**。请注意，这并不意味着我们必须始终使用相同的归约策略。例如，如果我们改变归约的元素数量，即使我们的归约策略改变，我们仍然可以保持批量不变性。Quack博客文章中有一些很好的例子，展示了你可以采用的各种归约策略的层次结构（例如，线程归约、线程束归约、块归约、集群归约）。

因此，只有当我们的批量大小影响归约策略时，才会破坏批量不变性。

让我们看看均方根归一化（RMSNorm）的标准并行策略。通常，并行算法受益于最小化核心之间的通信。为了本次讨论的目的，你可以假设当我们提到“核心”时，我们指的是流多处理器（SM）。更具体地说，这里重要的属性是我们的内核启动的线程块数量大于SM的数量。因此，我们可以开始采用的一种策略是为每个批量元素分配一个核心，如上所示。

增加我们的批量大小不会影响我们的归约策略；如果批量大小为200为我们的内核提供了足够的并行性，那么批量大小为2000肯定会提供足够的并行性。

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/rmsnorm-02.svg)

适用于更大批量的数据并行均方根归一化（RMSNorm）** 将数据并行策略扩展到更大的批量相当简单——不是让每个核心处理一行，而是允许每个核心按顺序处理不同的行。这**保持了批量不变性**，因为每个批量元素的归约策略保持相同。

另一方面，减小批量大小可能带来挑战。因为我们为每个批量元素分配一个核心，减小批量大小最终会导致核心数量多于批量元素数量，使一些核心闲置。

遇到这种情况时，优秀的内核工程师会采用前一节中提到的解决方案之一（原子加法或拆分归约），保持良好的并行性和性能。不幸的是，这会改变归约策略，导致该内核无法实现批量不变性。

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/rmsnorm-03.svg)

拆分归约的均方根归一化（RMSNorm）** 如果批量大小较小，我们的数据并行策略可能不再具有足够的并行性来使核心饱和。在这种情况下，“拆分”多个核心之间的归约可能更有效，从而充分利用GPU。然而，这**失去**了批量不变性，因为我们不再以相同的顺序归约每个元素。

最简单的解决方案是完全忽略这些情况。这并非完全**不合理**——小批量大小意味着内核无论如何都可能执行得很快，因此性能下降可能不会是灾难性的。

如果我们**必须**优化这种用例，一种方法是始终使用即使对于非常小的批量大小也具有足够并行性的归约策略。这样的归约策略对于较大的批量大小会导致过多的并行性，但可以让我们在整个大小范围内实现不错的（但不是峰值）性能。

#### 批量不变的矩阵乘法

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/matmul-01.svg)

数据并行矩阵乘法（Matmul）** 类似于均方根归一化（RMSNorm），矩阵乘法的标准并行策略是“数据并行”策略，将整个归约保留在一个核心内。最直接的想法是将输出张量拆分为2D块，并将每个块分配给不同的核心。然后，每个核心计算属于该块的点积，再次在一个核心内执行整个归约。

与均方根归一化（RMSNorm）不同，围绕算术强度和利用张量核的额外约束迫使我们拆分2D块而不是单个输出元素，以实现高效的矩阵乘法内核。

从本质上讲，你可以将矩阵乘法视为简单的逐点操作后跟一个归约。然后，如果我们通过将**输出**分块为块来并行化矩阵乘法，我们就有了类似的“数据并行”内核策略，使每个归约保持在一个核心内。

也类似于均方根归一化（RMSNorm），我们的“批量”维度（M和N）可能变得太小，迫使我们沿归约维度（K）拆分。尽管有两个“批量”维度，矩阵乘法也要求我们每个核心有更多的“工作”才能有效地利用张量核。例如，如果你有一个[1024, K] x [K, 1024]的矩阵乘法和标准的2D块大小[128, 128]，数据并行策略只能将这个矩阵乘法拆分为64个核心，不足以使GPU饱和。

在矩阵乘法中沿归约维度拆分称为拆分K矩阵乘法（Split-K Matmul）。与均方根归一化（RMSNorm）一样，使用这种策略会破坏批量不变性。
矩阵乘法的另一个有趣并行策略是流-k。流-k有趣是因为它比典型矩阵乘法的不变性甚至更低。如前所述，大多数矩阵乘法库不是批不变的，但它们至少可以称为批位置不变的（即改变批内元素的位置不影响数值）。然而，流-k也不是批位置不变的！其核心见解是，通过对不同的输出块沿k以不同方式拆分可以获得更干净的负载均衡，但利用这一点会使我们的内核也不是批位置不变的。

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/matmul-03.svg)

拆分-K矩阵乘法** 如果我们的批维度相当小，我们可能没有足够的并行性，需要拆分-k矩阵乘法。在这个例子中，我们将每次归约拆分为两个核心，它们会分别累加，然后在最后合并结果。然而，将每次归约拆分为两个核心仍允许我们利用八个核心。

矩阵乘法还有一个额外的复杂性——张量核心指令。而对于归约，我们可以一次简单地操作一行，高效的矩阵乘法内核必须一次操作整个“块”。

每个张量核心指令（比如`wgmma.mma_async.sync.aligned.m64n128k16`）内部可能有不同的归约顺序。使用不同张量核心指令的一个原因可能是批大小非常小。例如，如果我们使用一个操作长度为256的块的张量核心PTX指令，但批大小只有32，我们几乎浪费了所有的计算量！当批大小为1时，最快的内核通常根本不使用张量核心。

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/matmul-02.svg)

填充张量核心指令** 如果批大小太小，我们可能处于甚至无法在输出中容纳一个我们的二维块的情况。在这种情况下，最有效的是切换到更小的张量核心指令或完全放弃张量核心！然而，这两种选项都使我们的内核无法批不变。

所以，确保矩阵乘法批不变性的最简单方法是编译一个内核配置并用于所有形状。虽然我们会损失一些性能，但这在大语言模型推理中通常不是灾难性的。特别是，当M和N都很小时，拆分-k最需要，幸运的是在我们的案例中，N（即模型维度）通常相当大！

尽管获得了批不变性，与cuBLAS相比我们只损失了约20%的性能。注意这也不是优化的Triton内核（例如没有TMA）。然而，性能中的一些模式说明了我们的批不变性要求在哪里损失性能。首先，注意在非常小的批大小时，由于指令过大和并行性不足，我们损失了大量性能。其次，随着批大小增加出现“拼图”模式，这是由量化效应（块和波）引起的，通常通过改变块大小来改善。你可以在这里找到更多关于这些量化效应的信息。

#### 批不变性注意力

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-01.svg)

FlashAttention2策略** 我们沿Q并行化，同时沿K/V归约。这意味着我们的整个归约可以保持在单个核心内，使其成为另一种数据并行策略。

在获得矩阵乘法的批不变性后，注意力引入了两个额外的复杂点——恰如其分地，因为它包含两个矩阵乘法。

1. 与RMSNorm和矩阵乘法都只沿特征维度归约不同，我们现在沿特征维度和序列维度归约。
2. 由于上述原因，注意力必须处理各种影响序列处理方式的推理优化（分块预填充、前缀缓存等）。

因此，为了在大语言模型推理中实现确定性，我们的数值必须对同时处理的请求数量以及每个请求在推理引擎中如何切片都不变。

让我们首先回顾一下注意力的标准并行策略，首次在FlashAttention2中引入。与RMSNorm和矩阵乘法类似，默认策略是“数据并行”策略。由于我们沿键/值张量归约，数据并行策略只能沿查询张量并行化。

例如，根据推理引擎的选择，一个序列可能被分成几个部分处理（如分块预填充），或者可能一次性全部处理（如果预填充没有拆分）。为了实现“批不变性”，给定标记的归约顺序必须不依赖于其序列中同时处理的其他标记的数量。如果你将KV缓存中的KV值与当前处理的标记的KV值分开归约（如vLLM的Triton注意力内核），这无法实现。例如，在处理序列中的第1000个查询标记时，无论KV缓存中有0个标记（预填充）还是999个标记（解码），归约顺序必须相同。

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-02.svg)

带KV缓存的FlashAttention** 显式将KV缓存与当前KV值分开处理会破坏批不变性的原因有点微妙，与“边界条件”有关。特别是，想象你的块大小是32，但我们当前的KV缓存中有80个元素。然后我们计算另外48个未缓存的元素。在这种情况下，我们需要三个块（两个完整和一个掩码）来计算“P缓存”，另外两个块（一个完整和一个掩码）来计算“P”。因此，当我们只有总共四个块（即128）的元素要计算时，需要五个块来计算我们的归约，这肯定会改变我们的归约顺序。

例如，如果我们的KV缓存中没有元素且总共处理128个元素，我们需要在这两种情况下数值相同以确保注意力的“批不变性”。

为了解决这个问题，我们可以在注意力内核本身之前更新KV缓存和页表，确保我们的键和值始终一致布局，无论处理多少标记。

有了这个额外的细节（以及前一节提到的所有事情，如一致的块大小），我们能够实现批不变性的注意力实现！

然而，这里有一个重大问题。与矩阵乘法不同，我们在大语言模型推理中看到的注意力形状通常确实需要拆分-归约内核，通常称为Split-KV或FlashDecoding。这是因为如果我们不沿归约并行化，我们只能沿批维度、头维度和“查询长度”维度并行化。在注意力的解码阶段，查询长度非常小，所以除非我们有非常大的批大小，否则通常无法使GPU饱和。

不幸的是，像对RMSNorm和矩阵乘法那样忽略这种情况并不容易。例如，如果你有非常长的KV缓存，尽管只处理一个请求，注意力内核可能需要很长时间。

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-03.svg)

固定# Split-KV策略（即FlashDecode）** 如果我们的查询长度变得非常小（如解码时），我们可能最终处于内核中几乎没有并行性的情况。在这些情况下，我们需要再次沿归约维度拆分——这次是KV维度。沿KV维度拆分的典型策略是弄清楚我们需要多少并行性，然后均匀划分KV维度。例如，如果我们的KV长度是1000且需要4次拆分，每个核心将处理250个元素。

不幸的是，这也打破了批不变性，因为我们精确的归约策略取决于在任何给定请求中我们处理的序列中有多少查询标记。

此外，注意力常用的拆分-归约策略也给批不变性带来了挑战。例如，FlashInfer的“平衡调度算法”选择仍能让所有GPU核心饱和的最大拆分大小，从而使得归约策略不具有“批不变性”。然而，与RMSNorm/矩阵乘法不同的是，不能不管批大小都选择固定数量的拆分。

相反，为了实现批不变性，我们必须采用“固定拆分大小”策略。换句话说，不是固定拆分的数量，而是固定每个拆分的大小，最终得到不同数量的拆分。通过这种方式，我们可以保证无论处理多少标记，我们始终执行相同的归约顺序。这需要一些内部FlexAttention的更改，这些更改不包含在我们的代码发布中。我们将在不久的将来将它们上游提交！

![](/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-04.svg)

固定大小拆分KV策略**
该策略与之前策略的唯一区别是我们现在的拆分是“固定大小”的。例如，如果我们的KV长度是1000，不是将其分成四个长度为250的均匀拆分，而是分成三个固定大小长度为256的拆分和一个长度为232的拆分。

这使我们能够*保持*批不变性，因为我们的归约策略不再依赖于我们同时处理多少查询标记！

### 实现

我们通过利用vLLM的FlexAttention后端以及torch.Library，在vLLM之上提供确定性推理的演示。
通过torch.Library，我们能够以不侵入的方式替换大多数相关的PyTorch算子。你可以在thinking-machines-lab/batch-invariant-ops找到“批不变性”内核的库，以及vLLM在“确定性”模式下运行的示例。

### 实验

#### 完成结果的非确定性程度如何？

我们使用`Qwen/Qwen3-235B-A22B-Instruct-2507`，在温度0下，用提示“Tell me about Richard Feynman”采样1000个完成结果，每个生成1000个标记。令人惊讶的是，我们生成了80个唯一的完成结果，其中最常见的出现了78次。

查看完成结果不同的地方，我们发现前102个标记的完成结果实际上是相同的！第一个出现不同完成结果的标记是第103个。所有完成结果都生成序列“Feynman was born on May 11, 1918, in”，然而，992个完成结果继续生成“Queens, New York”，而8个完成结果生成“New York City”。

另一方面，当我们启用我们的批不变性内核时，我们所有的1000个完成结果都是相同的。这是我们的采样器在数学上期望的，但是没有我们的批不变性内核，我们无法实现确定性结果。

#### 性能

我们在这里没有投入大量精力优化批不变性内核的性能。然而，让我们进行一些实验来验证我们的性能仍然可用。

我们将设置一个API服务器，使用一个GPU运行Qwen-3-8B，并请求1000个序列，输出长度在90到110之间。

| Configuration                  | Time (seconds) |
| ------------------------------ | -------------- |
| vLLM default                   | 26             |
| Unoptimized Deterministic vLLM | 55             |
| + Improved Attention Kernel    | 42             |

大部分的性能下降来自于vLLM中的FlexAttention集成尚未得到大量优化。尽管如此，我们看到性能并没有*灾难性*。

#### 真正的在线策略强化学习

正如研究人员所指出的，训练和推理之间的不同数值隐含地将我们的在线策略强化学习转变为离线策略强化学习。

当然，如果我们甚至无法从两个相同的推理请求中获得逐位相同的结果，那么在训练和推理之间获得逐位相同的结果是不可能的。然后，确定性推理使我们能够修改我们的训练栈，以在采样和训练之间获得逐位相同的结果，从而实现真正的在线策略强化学习。

我们在Bigmath的RLVR设置中进行实验，RL策略从Qwen 2.5-VL instruct 8B初始化，最大展开长度为4096。

如果我们在没有离线策略校正（即重要性加权）的情况下训练，我们的奖励在训练过程中部分崩溃，而添加离线策略校正项可以使训练顺利进行。但是，如果我们在采样器和训练器之间实现逐位相同的结果，我们就完全处于在线策略（即KL散度为0），并且也可以顺利训练。

我们还可以绘制采样器和训练器之间对数概率的KL散度，其中3次运行有明显不同的行为。当使用重要性加权时，它保持在0.001左右，偶尔有峰值。然而，不使用重要性加权最终会在奖励崩溃的大致相同时间出现KL散度峰值。而且，当然，当运行“真正的在线策略强化学习”时，我们的KL散度保持在0不变，表明训练策略和采样策略之间没有*差异*。

注意，没有重要性加权的运行在第318步左右有显著的损失峰值，并且伴随着对数概率KL散度的相应峰值。同时，使用离线策略校正或运行“真正的在线策略”都可以使强化学习顺利进行。显示“真正的在线策略”的蓝线不是错误——它只是一条在0处的平线。

### 结论

现代软件系统包含多层抽象。在机器学习中，当我们遇到非确定性和细微的数值差异时，人们往往会倾向于掩盖它们。毕竟，我们的系统已经是“概率性的”，那么多一点非确定性有什么问题呢？在失败的单元测试中提高绝对误差/相对误差容限有什么问题呢？训练器和采样器之间对数概率的差异可能不是真正的错误，对吧？

我们拒绝这种失败主义。通过一点工作，我们*可以*理解我们非确定性的根本原因，甚至解决它们！我们希望这篇博客文章能让社区很好地理解如何解决我们推理系统中的非确定性，并激励其他人全面理解他们的系统。

### 引用

请引用这项工作：

```
He, Horace and Thinking Machines Lab, "Defeating Nondeterminism in LLM Inference",
Thinking Machines Lab: Connectionism, Sep 2025.
```
或使用BibTeX引用：

```
@article{he2025nondeterminism,
  author = {Horace He and Thinking Machines Lab},
  title = {Defeating Nondeterminism in LLM Inference},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  doi = {10.64434/tml.20250910}
}
```


---