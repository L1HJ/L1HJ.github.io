---
title: "Inside vLLM: Anatomy of a High-Throughput LLM Inference System - Aleksa Gordić"
source: "https://www.aleksagordic.com/blog/vllm#cpt2"
author:
  - "[[@gordic_aleksa]]"
published:
created: 2025-12-13
description: "From paged attention, continuous batching, prefix caching, specdec, etc. to multi-GPU, multi-node dynamic serving at scale."
tags:
  - "clippings"
---
在这篇文章中，我将逐步介绍构成现代高吞吐量LLM推理系统的所有核心系统组件和高级特性。特别是，我会详细分析vLLM [1] 的工作原理。 这篇文章是系列文章的第一篇。它从宏观入手，然后逐步深入细节（采用倒金字塔结构），让你在不被细枝末节淹没的情况下，对整个系统形成准确的高层认知模型。 后续文章将深入探讨特定子系统。 本文分为五个部分： 
1. [LLM引擎与引擎核心](https://www.aleksagordic.com/blog/#cpt1)：vLLM的基础（调度、分页注意力、连续批处理等） 
2. [高级特性](https://www.aleksagordic.com/blog/#cpt2)：分块预填充、前缀缓存、引导/投机解码、解耦预填充/解码 
3. [横向扩展](https://www.aleksagordic.com/blog/#cpt3)：从单GPU到多GPU执行 
4. [服务层](https://www.aleksagordic.com/blog/#cpt4)：分布式/并发Web架构 
5. [基准测试与自动调优](https://www.aleksagordic.com/blog/#cpt5)：测量延迟和吞吐量 


 ## LLM引擎与引擎核心 LLM引擎是vLLM的基础构建块。它本身已经支持高吞吐量推理——但仅在离线场景下。你还不能通过Web将其服务于客户。 我们将使用以下离线推理代码片段作为示例（改编自 [basic.py](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/basic.py)）。 """python from vllm import LLM, SamplingParams prompts = [ "Hello, my name is", "The president of the United States is", ] sampling_params = SamplingParams(temperature=0.8, top_p=0.95) def main(): llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0") outputs = llm.generate(prompts, sampling_params) if __name__ == "__main__": main() """ 📝 环境变量： 此配置为： - 离线模式（无Web/分布式系统架构） - 同步模式（所有执行在单个阻塞进程中进行） - 单GPU（无数据/模型/流水线/专家并行；DP/TP/PP/EP = 1） - 使用标准Transformer [2]（支持Jamba等混合模型需要更复杂的混合KV缓存内存分配器） 从这里开始，我们将逐步构建一个在线、异步、多GPU、多节点的推理系统——但仍服务于标准Transformer。 在这个示例中，我们做了两件事： 1. 实例化引擎 2. 调用 `generate` 方法从给定提示中采样 让我们开始分析构造函数。 ## LLM引擎构造函数 引擎的主要组件包括： - vLLM配置（包含所有用于配置模型、缓存、并行性等的旋钮） - 处理器（通过验证、分词和处理将原始输入转换为 `EngineCoreRequests`） - 引擎核心客户端（在示例中我们使用 `InprocClient`，它本质上等同于 `EngineCore`；我们将逐步构建到 `DPLBAsyncMPClient`，支持大规模服务） - 输出处理器（将原始 `EngineCoreOutputs` 转换为用户可见的 `RequestOutput`） 📝 说明： 引擎核心本身由几个子组件组成： - 模型执行器（驱动模型的前向传递，我们目前处理的是 `UniProcExecutor`，它在单个GPU上有一个 `Worker` 进程）。我们将逐步构建到 `MultiProcExecutor`，支持多个GPU。 - 结构化输出管理器（用于引导解码——稍后介绍） - 调度器（决定哪些请求进入下一个引擎步骤）——它进一步包含： 1. 策略设置——可以是 **FCFS**（先到先服务）或 **优先级**（高优先级请求优先处理） 2. `waiting` 和 `running` 队列 3. KV缓存管理器——分页注意力 [3] 的核心 KV缓存管理器维护一个 `free_block_queue`——可用KV缓存块的池（通常数量级为数十万，取决于显存大小和块大小）。在分页注意力中，块作为索引结构，将token映射到其计算出的KV缓存块。 ![LLM引擎构造函数](https://www.aleksagordic.com/blog/vllm/engine_constructor.png) 本节描述的核心组件及其关系 在模型执行器构造期间，会创建一个 `Worker` 对象，并执行三个关键步骤。（稍后，使用 `MultiProcExecutor` 时，这些步骤会在不同GPU上的每个worker进程中独立运行。） 1. 初始化设备： - 为worker分配CUDA设备（例如 "cuda:0"）并检查模型数据类型是否支持（例如bf16） - 验证是否有足够的显存（根据请求的 `gpu_memory_utilization`，例如0.8 → 总显存的80%） - 设置分布式设置（DP/TP/PP/EP等） - 实例化 `model_runner`（持有采样器、KV缓存和前向传递缓冲区，如 `input_ids`、`positions` 等） - 实例化 `InputBatch` 对象（持有CPU端前向传递缓冲区、用于KV缓存索引的块表、采样元数据等） 2. 加载模型： - 实例化模型架构 - 加载模型权重 - 调用 model.eval()（PyTorch的推理模式） - 可选：对模型调用 torch.compile() 3. 初始化KV缓存： - 获取每层KV缓存规格。历史上这总是 `FullAttentionSpec`（同构Transformer），但对于混合模型（滑动窗口、Transformer/SSM如Jamba），它变得更复杂（参见Jenga [5]） - 运行虚拟/ profiling前向传递并获取GPU内存快照，以计算可用显存中可容纳的KV缓存块数量 - 分配、重塑KV缓存张量并绑定到注意力层 - 准备注意力元数据（例如，将后端设置为FlashAttention），供内核在前向传递期间使用 - 除非提供 `--enforce-eager`，否则对每个预热批大小执行虚拟运行并捕获CUDA图。CUDA图将整个GPU工作序列记录为DAG。稍后在前向传递期间，我们启动/重放预烘焙的图，减少内核启动开销，从而改善延迟。 我在这里抽象了许多底层细节——但这些是我现在要介绍的核心部分，因为我将在后续章节中反复引用它们。 现在引擎已初始化，让我们继续探讨 `generate` 函数。 ## Generate函数 第一步是验证请求并将其输入引擎。对于每个提示： 1. 创建唯一请求ID并捕获其到达时间 2. 调用输入预处理器，对提示进行分词并返回包含 `prompt`、`prompt_token_ids` 和 `type`（文本、token、嵌入等）的字典 3. 将这些信息打包到 `EngineCoreRequest` 中，添加优先级、采样参数和其他元数据 4. 将请求传递给引擎核心，引擎核心将其包装在 `Request` 对象中并将其状态设置为 `WAITING`。然后将此请求添加到调度器的 `waiting` 队列（FCFS则追加，优先级则堆入） 此时引擎已接收请求，执行可以开始。在同步引擎示例中，这些初始提示是我们要处理的唯一请求——没有机制在运行中注入新请求。相比之下，异步引擎支持这一点（即 **连续批处理** [6]）：每一步后，都会考虑新请求和旧请求。 接下来，只要有请求要处理，引擎就会重复调用其 `step()` 函数。每个步骤包含三个阶段： 1. 调度：选择此步骤中要运行的请求（解码和/或分块预填充） 2. 前向传递：运行模型并采样token 3. 后处理：将采样的token ID附加到每个 `Request`，反分词，并检查停止条件。如果请求完成，清理（例如，将其KV缓存块返回 `free_block_queue`）并提前返回输出 📝 停止条件： ![引擎循环](https://www.aleksagordic.com/blog/vllm/engine_loop.png) 引擎循环 接下来，我们将更详细地检查调度。 ## 调度器 推理引擎处理两种主要类型的工作负载： 1. **预填充** 请求——对所有提示token的前向传递。这些通常是 **计算密集型**（阈值取决于硬件和提示长度）。最后，我们从最后一个token位置的概率分布中采样一个token。 2. **解码** 请求——仅对最新token的前向传递。所有早期KV向量已缓存。这些是 **内存带宽密集型**，因为我们仍然需要加载所有LLM权重（和KV缓存）来计算一个token。 V1调度器可以在同一步骤中混合这两种类型的请求，这要归功于更智能的设计选择。相比之下，V0引擎一次只能处理预填充或解码。 调度器优先处理解码请求： 1. 计算要生成的新token数量（由于投机解码和异步调度，不总是1——稍后介绍） 2. 调用KV缓存管理器的 `allocate_slots` 函数（细节如下） 3. 从步骤1的token数量中减去，更新token预算 之后，它处理来自 `waiting` 队列的预填充请求： 1. 获取计算出的块数量（如果禁用前缀缓存，则返回0——稍后介绍） 2. 调用KV缓存管理器的 `allocate_slots` 函数 3. 将请求从waiting队列弹出并移到running队列，将其状态设置为 `RUNNING` 4. 更新token预算 让我们看看 `allocate_slots` 做了什么： 1. **计算块数量**——确定必须分配的新KV缓存块数量（`n`）。每个块默认存储16个token。例如，如果预填充请求有17个新token，我们需要 `ceil(17/16) = 2` 个块。 2. **检查可用性**——如果管理器的池中没有足够的块，则提前退出。根据是解码还是预填充请求，引擎可能尝试重新计算抢占（V0支持交换抢占），通过驱逐低优先级请求（调用 `kv_cache_manager.free`，将KV块返回块池），或者可能跳过调度并继续执行。 3. **分配块**——通过KV缓存管理器的协调器，从块池（前面提到的 `free_block_queue` 双向链表）中获取前 `n` 个块。存储到 `req_to_blocks`，这是一个将每个 `request_id` 映射到其KV缓存块列表的字典。 ![KV缓存块](https://www.aleksagordic.com/blog/vllm/kv_cache_blocks.png) KV缓存块列表 我们终于准备好进行前向传递了！ ## 运行前向传递 我们调用模型执行器的 `execute_model`，它委托给 `Worker`，`Worker` 又委托给模型运行器。 主要步骤如下： 1. **更新状态**——从 `input_batch` 中修剪已完成的请求；更新与前向传递相关的元数据（例如，每个请求的KV缓存块，用于索引分页KV缓存内存） 2.**准备输入**——将缓冲区从CPU复制到GPU；计算位置；构建 `slot_mapping`（示例中详细介绍）；构造注意力元数据 3.**前向传递**——使用自定义分页注意力内核运行模型。所有序列被扁平化并连接成一个长的“超级序列”。位置索引和注意力掩码确保每个序列仅关注自己的token，这使得连续批处理无需右填充。 4.**收集最后一个token的状态**——提取每个序列最后位置的隐藏状态并计算logits 5.**采样**——根据采样配置（贪婪、温度、top-p、top-k等）从计算出的logits中采样token 前向传递步骤本身有两种执行模式： 1.** eager模式**——当启用eager执行时，运行标准PyTorch前向传递 2.**捕获模式**——当不强制eager时，执行/重放预捕获的CUDA图（记住我们在引擎构造期间的初始化KV缓存步骤中捕获了这些图） 以下是一个具体示例，清晰展示连续批处理和分页注意力： ![前向传递——连续批处理与分页注意力](https://www.aleksagordic.com/blog/vllm/fwd_pass.png) 前向传递：连续批处理与分页注意力 ## 高级特性——扩展核心引擎逻辑 有了基本引擎流程后，我们现在可以查看高级特性。 我们已经讨论了抢占、分页注意力和连续批处理。 接下来，我们将深入探讨： 1. 分块预填充 2. 前缀缓存 3. 引导解码（通过语法约束的有限状态机） 4. 投机解码 5. 解耦预填充/解码（P/D） ## 分块预填充 分块预填充是一种通过将预填充步骤拆分为较小块来处理长提示的技术。没有它，我们可能会遇到一个非常长的请求垄断一个引擎步骤，不允许其他预填充请求运行。这会推迟所有其他请求并增加它们的延迟。 例如，假设每个块包含 `n`（=8）个token，用小写字母表示并以"-"分隔。一个长提示 `P` 可能看起来像 `x-y-z`，其中 `z` 是不完整的块（例如2个token）。执行 `P` 的完整预填充将需要≥3个引擎步骤（如果在其中一个步骤中未调度，则可能更多），并且只有在最后一个分块预填充步骤中才会采样一个新token。 以下是该示例的可视化： ![分块预填充——第1部分](https://www.aleksagordic.com/blog/vllm/chunked_pt1.png) 实现很简单：限制每步的新token数量。如果请求的数量超过 `long_prefill_token_threshold`，则将其重置为该值。底层索引逻辑（前面介绍）会处理其余部分。 在vLLM V1中，你可以通过将 `long_prefill_token_threshold` 设置为正整数来启用分块预填充。（从技术上讲，无论此设置如何，如果提示长度超过token预算，它都会发生——我们会截断它并运行分块预填充。） ## 前缀缓存 为了解释前缀缓存的工作原理，让我们稍微调整原始代码示例： """python from vllm import LLM, SamplingParams long_prefix = "<a piece of text that is encoded into more than block_size tokens>" prompts = [ "Hello, my name is", "The president of the United States is", ] sampling_params = SamplingParams(temperature=0.8, top_p=0.95) def main(): llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0") outputs = llm.generate(long_prefix + prompts[0], sampling_params) outputs = llm.generate(long_prefix + prompts[1], sampling_params) if __name__ == "__main__": main() """ 前缀缓存避免重新计算多个提示开头共享的token——因此称为 **前缀**。 关键部分是 `long_prefix`：它被定义为任何长于KV缓存块（默认16个token）的前缀。为了简化示例，假设 `long_prefix` 的长度恰好是 `n x block_size`（其中 `n ≥1`）。 没有前缀缓存，每次我们处理带有相同 `long_prefix` 的新请求时，都会重新计算所有 `n x block_size` 个token。 有了前缀缓存，这些token只需计算一次（它们的KV存储在KV缓存分页内存中），然后重用，因此只需处理新的提示token。这加快了预填充请求（但对解码没有帮助）。 这在vLLM中如何工作？ 在第一次 `generate` 调用期间，在调度阶段，`kv_cache_manager.get_computed_blocks` 内部，引擎调用 `hash_request_tokens`： 1. 此函数将 `long_prefix + prompts[0]` 拆分为16-token块 2. 对于每个完整块，计算哈希（使用内置哈希或SHA-256，后者较慢但冲突较少）。哈希结合了前一个块的哈希、当前token和可选元数据 3. 每个结果存储为 `BlockHash` 对象，包含哈希及其token ID。我们返回块哈希列表 该列表存储在 `self.req_to_block_hashes[request_id]` 中。 接下来，引擎调用 `find_longest_cache_hit` 来检查这些哈希是否已存在于 `cached_block_hash_to_block` 中。在第一个请求中，没有找到命中。 ![前缀缓存逻辑——第1部分](https://www.aleksagordic.com/blog/vllm/prefix_pt1.png) 然后我们调用 `allocate_slots`，它调用 `coordinator.cache_blocks`，将新的 `BlockHash` 条目与分配的KV块相关联，并将它们记录在 `cached_block_hash_to_block` 中。 之后，前向传递将在与我们上面分配的KV缓存块对应的分页KV缓存内存中填充KV。 ![前缀缓存逻辑——第2部分](https://www.aleksagordic.com/blog/vllm/prefix_pt2.png) 在第二次使用相同前缀的 `generate` 调用中，步骤1-3重复，但现在 `find_longest_cache_hit` 找到了所有 `n` 个块的匹配项（通过线性搜索）。引擎可以直接重用这些KV块。 ![前缀缓存逻辑——第3部分](https://www.aleksagordic.com/blog/vllm/prefix_pt3.png) 如果原始请求仍然有效，则这些块的引用计数将增加（例如到2）。在此示例中，第一个请求已完成，因此块被释放回池，其引用计数设置回0。因为我们能够从 `cached_block_hash_to_block` 中检索它们，所以我们知道它们是有效的（KV缓存管理器的逻辑是这样设置的），因此我们只需再次将它们从 `free_block_queue` 中移除。 📝 高级说明： 这就是前缀缓存的要点：不要重新计算已经见过的前缀——只需重用它们的KV缓存！ 前缀缓存默认启用。要禁用它：`enable_prefix_caching = False`。 ## 引导解码（FSM） 引导解码是一种技术，在每个解码步骤中，logits受基于语法的有限状态机（FSM）约束。这确保只有语法允许的token才能被采样。 这是一个强大的设置：你可以强制执行从正则语法（乔姆斯基3型，例如任意正则表达式模式）到上下文无关语法（2型，涵盖大多数编程语言）的任何内容。 为了使其不那么抽象，让我们从最简单的示例开始，基于前面的代码： """python from vllm import LLM, SamplingParams from vllm.sampling_params import GuidedDecodingParams prompts = [ "This sucks", "The weather is beautiful", ] guided_decoding_params = GuidedDecodingParams(choice=["Positive", "Negative"]) sampling_params = SamplingParams(guided_decoding=guided_decoding_params) def main(): llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0") outputs = llm.generate(prompts, sampling_params) if __name__ == "__main__": main() """ 在我给出的玩具示例中（假设字符级分词）：预填充时，FSM会屏蔽logits，因此只有“P”或“N”是可行的。如果采样到“P”，FSM会移动到“Positive”分支；下一步只允许“o”，依此类推。 ![FSM](https://www.aleksagordic.com/blog/vllm/fsm.png) 玩具示例FSM 这在vLLM中如何工作？ 1. 在LLM引擎构造期间，创建 `StructuredOutputManager`；它有权访问分词器并维护 `_grammar_bitmask` 张量 2. 添加请求时，其状态设置为 `WAITING_FOR_FSM`，`grammar_init` 选择后端编译器（例如 `xgrammar` [7]；注意后端是第三方代码） 3. 此请求的语法异步编译 4. 在调度期间，如果异步编译已完成，则状态切换为 `WAITING`，`request_id` 添加到 `structured_output_request_ids`；否则将其放入 `skipped_waiting_requests`，以便在下一个引擎步骤中重试 5. 在调度循环结束时（仍在调度内部），如果有FSM请求，则 `StructuredOutputManager` 要求后端准备/更新 `_grammar_bitmask` 6. 前向传递产生logits后，xgr_torch_compile的函数将位掩码扩展到词汇表大小（因为我们使用32位整数，所以扩展比为32x），并将不允许的logits掩码为-∞ 7. 采样下一个token后，通过 `accept_tokens` 推进请求的FSM。从视觉上看，我们移动到FSM图上的下一个状态 步骤6值得进一步澄清。 如果 `vocab_size =32`，`_grammar_bitmask` 是一个整数；其二进制表示编码哪些token是允许的（“1”）vs不允许的（“0”）。例如，“101…001”扩展为长度为32的数组 `[1,0,1,…,0,0,1]`；位置为0的logits设置为-∞。对于更大的词汇表，使用多个32位字并相应扩展/连接。后端（例如 `xgrammar`）负责使用当前FSM状态生成这些位模式。 📝 说明： 以下是一个更简单的示例，词汇表大小为8，使用8位整数（适合视觉展示）： ![FSM](https://www.aleksagordic.com/blog/vllm/fsm2.png) 玩具示例 你可以通过传入所需的 `guided_decoding` 配置在vLLM中启用此功能。 ## 投机解码 在自回归生成中，每个新token都需要大型LM的前向传递。这很昂贵——每一步都要重新加载并应用所有模型权重，只为了计算一个token！（假设批大小为1，通常是 `B`） 投机解码 [8] 通过引入一个较小的草稿LM来加速这一过程。草稿LM廉价地提出 `k` 个token。但我们最终不想从较小的模型中采样——它只是用来猜测候选延续。大型模型仍然决定什么是有效的。 步骤如下： 1. **草稿**：在当前上下文上运行小型模型并提出 `k` 个token 2. **验证**：在上下文 + `k` 个草稿token上运行一次大型模型。这会产生这些 `k` 个位置的概率，再加上一个额外的（因此我们得到 `k+1` 个候选） 3. **接受/拒绝**：从左到右遍历 `k` 个草稿token： - 如果大型模型对草稿token的概率≥草稿模型的概率，则接受它 - 否则，以概率 `p_large(token)/p_draft(token)` 接受它 - 在第一次拒绝时停止，或接受所有 `k` 个草稿token - 如果所有 `k` 个草稿token都被接受，则还可以从大型模型中“免费”采样额外的第 `k+1` 个token（我们已经计算了该分布） - 如果有拒绝，则在该位置创建一个新的重新平衡分布（`p_large - p_draft`，最小值钳位为0，归一化为总和1），并从该分布中采样最后一个token **为什么这有效？** 尽管我们使用小型模型提出候选，但接受/拒绝规则保证在期望中，序列的分布与我们从大型模型中逐个token采样的分布完全相同。这意味着投机解码在统计上等同于标准自回归解码——但可能快得多，因为一次大型模型传递可以产生多达 `k+1` 个token。 📝 说明： vLLM V1不支持LLM草稿模型方法，而是实现了更快但准确性较低的提议方案：n-gram、EAGLE [9] 和 Medusa [10]。 每个方案的一句话说明： 1. **n-gram**：取最后 `prompt_lookup_max` 个token；在序列中找到先前的匹配；如果找到，则提出跟随该匹配的 `k` 个token；否则递减窗口并重试，直到 `prompt_lookup_min` 2. **EAGLE**：对大型LM进行“模型手术”——保留嵌入和LM头，将Transformer栈替换为轻量级MLP；微调它作为廉价草稿 3. **Medusa**：在大型模型的顶部（LM头之前的嵌入）训练辅助线性头，并行预测下一个 `k` 个token；使用这些头比运行单独的小型LM更高效地提出token 以下是如何在vLLM中使用 `ngram` 作为草稿方法调用投机解码： """python from vllm import LLM, SamplingParams prompts = [ "Hello, my name is", "The president of the United States is", ] sampling_params = SamplingParams(temperature=0.8, top_p=0.95) speculative_config={ "method": "ngram", "prompt_lookup_max": 5, "prompt_lookup_min": 3, "num_speculative_tokens": 3, } def main(): llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", speculative_config=speculative_config) outputs = llm.generate(prompts, sampling_params) if __name__ == "__main__": main() """ 这在vLLM中如何工作？ **设置（引擎构造期间）：** 1. 初始化设备：创建 `drafter`（草稿模型，例如 `NgramProposer`）和 `rejection_sampler`（部分用Triton编写） 2. 加载模型：加载草稿模型权重（n-gram无需操作） **之后在 `generate` 函数中**（假设我们收到一个全新的请求）： 1. 使用大型模型运行常规预填充步骤 2. 在前向传递和标准采样后，调用 `propose_draft_token_ids(k)` 从草稿模型中采样 `k` 个草稿token 3. 将这些存储在 `request.spec_token_ids` 中（更新请求元数据） 4. 在下一个引擎步骤中，当请求在运行队列中时，将 `len(request.spec_token_ids)` 添加到“新token”计数中，以便 `allocate_slots` 为前向传递保留足够的KV块 5. 将 `spec_token_ids` 复制到 `input_batch.token_ids_cpu` 中，形成（上下文 + 草稿）token 6. 通过 `_calc_spec_decode_metadata` 计算元数据（这会从 `input_batch.token_ids_cpu` 复制token，准备logits等），然后在草稿token上运行大型模型前向传递 7. 不是从logits中常规采样，而是使用 `rejection_sampler` 从左到右接受/拒绝并产生 `output_token_ids` 8. 重复步骤2-7，直到满足停止条件 最好的理解方法是启动调试器并逐步浏览代码库，但本节希望能让你对它有一个初步了解。还有这些： ![草稿阶段](https://www.aleksagordic.com/blog/vllm/specdec_pt1.png) ![验证阶段与拒绝采样阶段](https://www.aleksagordic.com/blog/vllm/specdec_pt2.png) ## 解耦预填充/解码（P/D） 我之前已经暗示了解耦预填充/解码（P/D）背后的动机。 预填充和解码具有非常不同的性能特征（计算密集型vs内存带宽密集型），因此分离它们的执行是一个合理的设计。它能更严格地控制延迟——包括 `TFTT`（首token时间）和 `ITL`（token间延迟）——更多信息在 [基准测试](https://www.aleksagordic.com/blog/#cpt5) 部分。 在实践中，我们运行 `N` 个vLLM预填充实例和 `M` 个vLLM解码实例，根据实时请求混合自动缩放它们。预填充worker将KV写入专用KV缓存服务；解码worker从该服务读取。这将长而突发的预填充与稳定、对延迟敏感的解码隔离开来。 这在vLLM中如何工作？ 为了清晰起见，以下示例依赖于 `SharedStorageConnector`，这是一个用于说明机制的调试连接器实现。 我们启动2个vLLM实例（GPU 0用于预填充，GPU 1用于解码），然后在它们之间传输KV缓存： """python import os import time from multiprocessing import Event, Process import multiprocessing as mp from vllm import LLM, SamplingParams from vllm.config import KVTransferConfig prompts = [ "Hello, my name is", "The president of the United States is", ] def run_prefill(prefill_done): os.environ["CUDA_VISIBLE_DEVICES"] = "0" sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=1) ktc=KVTransferConfig( kv_connector="SharedStorageConnector", kv_role="kv_both", kv_connector_extra_config={"shared_storage_path": "local_storage"}, ) llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", kv_transfer_config=ktc) llm.generate(prompts, sampling_params) prefill_done.set() # 通知解码实例KV缓存已准备好 # 保持预填充节点运行以防解码节点未完成；否则脚本可能过早退出，导致解码不完整 try: while True: time.sleep(1) except KeyboardInterrupt: print("脚本被用户停止。") def run_decode(prefill_done): os.environ["CUDA_VISIBLE_DEVICES"] = "1" sampling_params = SamplingParams(temperature=0, top_p=0.95) ktc=KVTransferConfig( kv_connector="SharedStorageConnector", kv_role="kv_both", kv_connector_extra_config={"shared_storage_path": "local_storage"}, ) llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", kv_transfer_config=ktc) prefill_done.wait() # 阻塞等待预填充实例的KV缓存 # 内部会先获取KV缓存，然后开始解码循环 outputs = llm.generate(prompts, sampling_params) if __name__ == "__main__": prefill_done = Event() prefill_process = Process(target=run_prefill, args=(prefill_done,)) decode_process = Process(target=run_decode, args=(prefill_done,)) prefill_process.start() decode_process.start() decode_process.join() prefill_process.terminate() """ vLLM中的步骤如下： 1. **实例化**——在引擎构造期间，连接器在两个地方创建： - 在worker的初始化设备步骤中（在初始化worker分布式环境函数下），角色为“worker” - 在调度器构造函数中，角色为“scheduler” 2. **缓存查找**——当调度器处理 `waiting` 队列中的预填充请求时（本地前缀缓存检查后），它调用连接器的 `get_num_new_matched_tokens`。这会检查KV缓存服务器中的外部缓存token。预填充在此处始终看到0；解码可能有缓存命中。结果添加到本地计数中，然后调用 `allocate_slots` 3. **状态更新**——调度器然后调用 `connector.update_state_after_alloc`，记录有缓存的请求（预填充无需操作） 4. **元数据构建**——在调度结束时，调度器调用 `meta = connector.build_connector_meta`： - 预填充添加所有 `is_store=True` 的请求（用于上传KV） - 解码添加所有 `is_store=False` 的请求（用于获取KV） 5. **上下文管理器**——在前向传递之前，引擎进入KV连接器上下文管理器： - 进入时：调用 `kv_connector.start_load_kv`。对于解码，这会从外部服务器加载KV并将其注入分页内存。对于预填充，无需操作。 - 退出时：调用 `kv_connector.wait_for_save`。对于预填充，这会阻塞直到KV上传到外部服务器。对于解码，无需操作。 以下是可视化示例： ![解耦P/D](https://www.aleksagordic.com/blog/vllm/pd.png) 解耦P/D ## 从UniProcExecutor到MultiProcExecutor 有了核心技术后，我们现在可以讨论横向扩展。 假设你的模型权重不再适合单个GPU的显存。 第一个选项是使用张量并行（例如 `TP=8`）在同一节点的多个GPU上分片模型。如果模型仍然不适合，下一步是跨节点的流水线并行。 📝 说明： 在这个阶段，我们需要多个GPU进程（worker）和一个协调层来协调它们。这正是 `MultiProcExecutor` 提供的。 ![MultiProcExecutor](https://www.aleksagordic.com/blog/vllm/multiprocexecutor.png) TP=8设置中的MultiProcExecutor（驱动worker为rank 0） 这在vLLM中如何工作？ 1. `MultiProcExecutor` 初始化一个 `rpc_broadcast_mq` 消息队列（底层使用共享内存实现） 2. 构造函数循环遍历 `world_size`（例如 `TP=8 ⇒ world_size=8`），并通过 `WorkerProc.make_worker_process` 为每个rank生成一个守护进程 3. 对于每个worker，父进程首先创建一个读写管道 4. 新进程运行 `WorkerProc.worker_main`，实例化一个worker（与 `UniProcExecutor` 中的步骤相同：初始化设备、加载模型等） 5. 每个worker确定自己是驱动（TP组中的rank 0）还是常规worker。每个worker设置两个队列： - `rpc_broadcast_mq`（与父进程共享）用于接收工作 - `worker_response_mq` 用于发送响应 6. 在初始化期间，每个子进程通过管道将其 `worker_response_mq` 句柄发送给父进程。一旦所有句柄都收到，父进程解除阻塞——这完成了协调 7. worker然后进入忙循环，阻塞在 `rpc_broadcast_mq.dequeue`。当工作项到达时，它们执行它（与 `UniProcExecutor` 相同，但现在有TP/PP特定的分区工作）。结果通过 `worker_response_mq.enqueue` 发送回 8. 在运行时，当请求到达时，`MultiProcExecutor` 将其非阻塞地入队到所有子worker的 `rpc_broadcast_mq`。然后它等待指定输出rank的 `worker_response_mq.dequeue` 以收集最终结果 从引擎的角度来看，没有任何变化——所有这些多进程复杂性都通过调用模型执行器的 `execute_model` 抽象出来。 - 在 `UniProcExecutor` 情况下：execute_model直接调用worker的execute_model - 在 `MultiProcExecutor` 情况下：execute_model通过 `rpc_broadcast_mq` 间接调用每个worker的execute_model 此时，我们可以使用相同的引擎接口运行尽可能大的模型（只要资源允许）。 下一步是向外扩展：启用数据并行（`DP>1`），跨节点复制模型，添加轻量级DP协调层，跨副本引入负载均衡，并在前面放置一个或多个API服务器来处理传入流量。 ## 分布式系统服务vLLM 有许多方法可以设置服务基础设施，但为了具体起见，这里有一个示例：假设我们有两个H100节点，想要在它们上运行四个vLLM引擎。 如果模型需要 `TP=4`，我们可以这样配置节点。 ![2个8xH100节点的服务器配置](https://www.aleksagordic.com/blog/vllm/server_setup.png) 2个8xH100节点的服务器配置（1个无头节点，1个API服务器） 在第一个节点上，以无头模式（无API服务器）运行引擎，参数如下： """python vllm serve <model-name> --tensor-parallel-size 4 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-start-rank 0 --data-parallel-address <master-ip> --data-parallel-rpc-port 13345 --headless """ 在另一个节点上运行相同的命令，只需稍作调整： - 无 `--headless` - 修改DP起始rank """python vllm serve <model-name> --tensor-parallel-size 4 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-start-rank 2 --data-parallel-address <master-ip> --data-parallel-rpc-port 13345 """ 📝 说明： 这在vLLM中如何工作？ ## 在无头服务器节点上 在无头节点上，`CoreEngineProcManager` 启动2个进程（每个 `--data-parallel-size-local`），每个进程运行 `EngineCoreProc.run_engine_core`。每个函数创建一个 `DPEngineCoreProc`（引擎核心），然后进入忙循环。 `DPEngineCoreProc` 初始化其父 `EngineCoreProc`（`EngineCore` 的子进程）： 1. 创建 `input_queue` 和 `output_queue`（`queue.Queue`） 2. 使用 `DEALER` ZMQ套接字（异步消息库）与另一个节点上的前端执行初始握手，并接收协调地址信息 3. 初始化DP组（例如使用NCCL后端） 4. 用 `MultiProcExecutor` 初始化 `EngineCore`（如前所述，在4个GPU上 `TP=4`） 5. 创建 `ready_event`（`threading.Event`） 6. 启动一个输入守护线程（`threading.Thread`），运行 `process_input_sockets(…, ready_event)`。同样启动一个输出线程 7. 仍在主线程中，等待 `ready_event`，直到所有4个进程（跨2个节点）的所有输入线程完成协调握手，最终执行 `ready_event.set()` 8. 解除阻塞后，向前端发送“就绪”消息，包含元数据（例如分页KV缓存内存中可用的 `num_gpu_blocks`） 9. 主线程、输入线程和输出线程然后进入各自的忙循环 TL;DR：我们最终得到4个子进程（每个DP副本一个），每个进程运行主线程、输入线程和输出线程。它们与DP协调器和前端完成协调握手，然后每个进程的三个线程在稳态忙循环中运行。 ![有4个DPEngineCoreProc的分布式系统](https://www.aleksagordic.com/blog/vllm/dpenginecoreproc.png) 有4个DP副本运行4个DPEngineCoreProc的分布式系统 **当前稳态：** - **输入线程**——阻塞在输入套接字上，直到API服务器路由请求；收到后，解码有效负载，通过 `input_queue.put_nowait(...)` 入队工作项，然后返回阻塞在套接字上 - **主线程**——在 `input_queue.get(...)` 上唤醒，将请求输入引擎；`MultiProcExecutor` 运行前向传递并将结果入队到 `output_queue` - **输出线程**——在 `output_queue.get(...)` 上唤醒，将结果发送回API服务器，然后继续阻塞 **附加机制：** - **DP wave计数器**——系统跟踪“wave”；当所有引擎空闲时，它们会静止，当新工作到达时计数器递增（用于协调/指标） - **控制消息**——API服务器可以发送不仅仅是推理请求（例如中止和实用/控制RPC） - **锁步的虚拟步骤**——如果任何DP副本有工作，所有副本都执行前向步骤；没有请求的副本执行虚拟步骤以参与所需的同步点（避免阻塞活动副本） 现在是第二部分，API服务器节点上发生了什么？ ## 在API服务器节点上 我们实例化一个 `AsyncLLM` 对象（LLM引擎的asyncio包装器）。内部创建一个 `DPLBAsyncMPClient`（数据并行、负载均衡、异步、多进程客户端）。 在 `MPClient` 的父类中，`launch_core_engines` 函数运行： 1. 创建用于启动握手的ZMQ地址（如无头节点上所见） 2. 启动 `DPCoordinator` 进程 3. 创建 `CoreEngineProcManager`（与无头节点相同） 在 `AsyncMPClient`（`MPClient` 的子类）中： 1. 创建 `outputs_queue`（`asyncio.Queue`） 2. 创建一个asyncio任务 `process_outputs_socket`，它与所有4个 `DPEngineCoreProc` 的输出线程通信（通过输出套接字）并写入 `outputs_queue` 3. 随后，`AsyncLLM` 的另一个asyncio任务 `output_handler` 从这个队列读取，最终将信息发送到 `create_completion` 函数 在 `DPAsyncMPClient` 中，我们创建一个asyncio任务 `run_engine_stats_update_task`，它与DP协调器通信。 DP协调器在前端（API服务器）和后端（引擎核心）之间进行调解： - 定期将负载均衡信息（队列大小、等待/运行请求）发送到前端的 `run_engine_stats_update_task` - 处理来自前端的 `SCALE_ELASTIC_EP` 命令，动态更改引擎数量（仅适用于Ray后端） - 向前端触发的后端发送 `START_DP_WAVE` 事件，并将wave状态更新报告回前端 回顾一下，前端（`AsyncLLM`）运行几个asyncio任务（记住：并发，而非并行）： - 一类任务通过 `generate` 路径处理输入请求（每个新客户端请求生成一个新的asyncio任务） - 两个任务（`process_outputs_socket`、`output_handler`）处理来自底层引擎的输出消息 - 一个任务（`run_engine_stats_update_task`）与DP协调器通信：发送wave触发、轮询LB状态、处理动态缩放请求 最后，主服务器进程创建一个FastAPI应用程序并挂载端点，如 `OpenAIServingCompletion` 和 `OpenAIServingChat`，它们公开 `/completion`、`/chat/completion` 等。然后通过Uvicorn服务此栈。 总结一下，完整的请求生命周期如下！ 你从终端发送： """bash curl -X POST http://localhost:8000/v1/completions -H "Content-Type: application/json" -d '{ "model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "prompt": "The capital of France is", "max_tokens": 50, "temperature": 0.7 }' """ 接下来发生什么： 1. 请求到达API服务器上的 `OpenAIServingCompletion` 的 `create_completion` 路由 2. 函数异步对提示进行分词，并准备元数据（请求ID、采样参数、时间戳等） 3. 然后调用 `AsyncLLM.generate`，它遵循与同步引擎相同的流程，最终调用 `DPAsyncMPClient.add_request_async` 4. 这反过来调用 `get_core_engine_for_request`，根据DP协调器的状态（选择得分最小/负载最低的引擎：`score = len(waiting)*4 + len(running)`）在引擎之间进行负载均衡 5. `ADD` 请求发送到所选引擎的 `input_socket` 6. 在该引擎上： - 输入线程——解除阻塞，从输入套接字解码数据，并将工作项放置在 `input_queue` 中供主线程处理 - 主线程——在 `input_queue` 上唤醒，将请求输入引擎；`MultiProcExecutor` 运行前向传递并将结果入队到 `output_queue` - 输出线程——在 `output_queue` 上唤醒，并通过输出套接字将结果发送回 7. 这些结果触发 `AsyncLLM` 输出asyncio任务（`process_outputs_socket` 和 `output_handler`），它们将token传播回FastAPI的 `create_completion` 路由 8. FastAPI附加元数据（完成原因、logprobs、使用信息等），并通过Uvicorn将 `JSONResponse` 返回给你的终端！ 就这样，你的完成结果回来了——整个分布式机制隐藏在一个简单的 `curl` 命令后面！:) 太有趣了！！！ 📝 附加说明： ## 基准测试与自动调优——延迟vs吞吐量 到目前为止，我们一直在分析“气体粒子”——请求如何流经引擎/系统的内部。现在是时候放大，查看整个系统，并问：我们如何衡量推理系统的性能？ 在最高级别，有两个相互竞争的指标： 1. **延迟**——从请求提交到token返回的时间 2. **吞吐量**——系统每秒可以生成/处理的token/请求数量 **延迟** 对于交互式应用最重要，用户正在等待响应。 **吞吐量** 在离线工作负载中很重要，例如预训练/后训练运行的合成数据生成、数据清理/处理，以及一般来说——任何类型的离线批处理推理作业。 在解释为什么延迟和吞吐量相互竞争之前，让我们定义一些常见的推理指标： | 指标 | 定义 | | --- | --- | | `TTFT`（首token时间） | 从请求提交到收到第一个输出token的时间 | | `ITL`（token间延迟） | 两个连续token之间的时间（例如，从token i-1到token i） | | `TPOT`（每个输出token的时间） | 请求中所有输出token的平均ITL | | `延迟/E2E`（端到端延迟） | 处理请求的总时间，即TTFT + 所有ITL的总和，或者等效于提交请求和收到最后一个输出token之间的时间 | | `吞吐量` | 每秒处理的总token数（输入、输出或两者），或者请求每秒 | | `有效吞吐量` | 满足服务水平目标（SLO）的吞吐量，例如最大TTFT、TPOT或端到端延迟。例如，仅计算满足这些SLO的请求的token | ![TTFT、ITL、端到端延迟](https://www.aleksagordic.com/blog/vllm/latency_diagram.png) TTFT、ITL、端到端延迟 以下是一个简化模型，解释这两个指标的竞争性质。 当查看批大小 `B` 如何影响单个解码步骤时，权衡变得清晰。随着 `B` 向1减小，ITL降低：每步工作更少，token不与其他token“竞争”。随着 `B` 向无穷大增加，ITL上升，因为每步执行更多FLOP——但吞吐量提高（直到达到峰值性能），因为权重I/O在更多token上摊销。 屋顶线模型有助于理解这一点：在饱和批大小 `B_sat` 以下，步骤时间由HBM带宽主导（逐层将权重流式传输到片上内存），因此步骤延迟几乎平坦——计算1个vs 10个token可能需要相似的时间。超过 `B_sat`，内核变得计算密集型，步骤时间大致随 `B` 增长；每个额外token都会增加ITL。 ![屋顶线性能模型](https://www.aleksagordic.com/blog/vllm/roofline.png) 屋顶线性能模型 📝 说明： ## 如何在vLLM中进行基准测试 vLLM提供 `vllm bench {serve,latency,throughput}` CLI，它包装了vllm/benchmarks/{server,latency,throughput}.py。 这些脚本的作用： - **latency**——使用短输入（默认32个token）并以小批（默认8）采样128个输出token。它运行几次迭代并报告批的端到端延迟。 - **throughput**——一次性提交一组固定的提示（默认：1000个ShareGPT样本）（即 `QPS=Inf` 模式），并报告整个运行期间的输入/输出/总token数和请求每秒。 - **serve**——启动vLLM服务器并通过从泊松（或更一般地说，伽马）分布中采样请求到达时间来模拟真实世界的工作负载。它在时间窗口内发送请求，测量我们讨论过的所有指标，并且可以选择强制服务器端最大并发（通过信号量，例如将服务器限制为64个并发请求）。 以下是运行延迟脚本的示例： """bash vllm bench latency --model <model-name> --input-tokens 32 --output-tokens 128 --batch-size 8 """ 还有一个自动调优脚本，驱动serve基准测试以找到满足目标SLO的参数设置（例如，“在保持p99端到端<500 ms的同时最大化吞吐量”），返回建议的配置。 ## 结语 我们从基本引擎核心（`UniProcExecutor`）开始，添加了投机解码和前缀缓存等高级特性，扩展到 `MultiProcExecutor`（`TP/PP>1`），最后向外扩展，将所有内容包装在异步引擎和分布式服务栈中——最后介绍如何衡量系统性能。 vLLM还包括我跳过的专门处理。例如： - **多样化硬件后端**：TPU、AWS Neuron（Trainium/Inferentia）等 - **架构/技术**：`MLA`、`MoE`、编码器-解码器（例如Whisper）、池化/嵌入模型、`EPLB`、`m-RoPE`、`LoRA`、`ALiBi`、无注意力变体、滑动窗口注意力、多模态LLM和状态空间模型（例如Mamba/Mamba-2、Jamba） - **TP/PP/SP** - **混合KV缓存逻辑**（Jenga）、更复杂的采样方法如束采样等 - **实验性**：异步调度 好消息是，大多数这些与上面描述的主要流程正交——你几乎可以将它们视为“插件”（当然，在实践中存在一些耦合）。 我喜欢理解系统。话虽如此，在这个高度上，分辨率肯定受到了影响。在后续文章中，我将深入探讨特定子系统，进入细节。 💡 联系我： ## 致谢 非常感谢 [Hyperstack](https://www.hyperstack.cloud/) 在过去一年中为我的实验提供H100！ 感谢 [Nick Hill](https://www.linkedin.com/in/nickhillprofile/)（核心vLLM贡献者，RedHat）、[Mark Saroufim](https://x.com/marksaroufim)（PyTorch）、[Kyle Krannen](https://www.linkedin.com/in/kyle-kranen/)（NVIDIA，Dynamo）和 [Ashish Vaswani](https://www.linkedin.com/in/ashish-vaswani-99892181/) 阅读这篇博客文章的预发布版本并提供反馈！ ## 参考文献 1. vLLM [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm) 2. "Attention Is All You Need", [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762) 3. "Efficient Memory Management for Large Language Model Serving with PagedAttention", [https://arxiv.org/abs/2309.06180](https://arxiv.org/abs/2309.06180) 4. "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model", [https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434) 5. "Jenga: Effective Memory Management for Serving LLM with Heterogeneity", [https://arxiv.org/abs/2503.18292](https://arxiv.org/abs/2503.18292) 6. "Orca: A Distributed Serving System for Transformer-Based Generative Models", [https://www.usenix.org/conference/osdi22/presentation/yu](https://www.usenix.org/conference/osdi22/presentation/yu) 7. "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models", [https://arxiv.org/abs/2411.15100](https://arxiv.org/abs/2411.15100) 8. "Accelerating Large Language Model Decoding with Speculative Sampling", [https://arxiv.org/abs/2302.01318](https://arxiv.org/abs/2302.01318) 9. "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty", [https://arxiv.org/abs/2401.15077](https://arxiv.org/abs/2401.15077) 10. "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads", [https://arxiv.org/abs/2401.10774](https://arxiv.org/abs/2401.10774) 11. LMCache, [https://github.com/LMCache/LMCache](https://github.com/LMCache/LMCache)