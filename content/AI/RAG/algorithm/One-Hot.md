## 1. 引言
在自然语言处理（NLP）和机器学习中，**One-Hot 编码** 是最基础的离散特征向量表示方法之一。  
它将离散类别映射为向量，每个类别对应一个维度，值为 1 表示该类别出现，其他维度为 0。  
One-Hot 是很多现代嵌入方法（如 Word Embedding、Embedding Table）的基础。

---

## 2. One-Hot 编码原理

假设我们有一个词汇表：

$$
V = \{ \text{“apple”}, \text{“banana”}, \text{“orange”} \}
$$

每个词可以表示为一个 3 维向量：

| 词 | One-Hot 向量 |
|----|---------------|
| apple  | $[1, 0, 0]$ |
| banana | $[0, 1, 0]$ |
| orange | $[0, 0, 1]$ |

公式表示为：

$$
\text{one\_hot}(x) = [0, \dots, 0, 1, 0, \dots, 0] \in \mathbb{R}^{|V|}
$$

其中 1 出现在类别 $x$ 对应的索引位置。

---

## 3. 特点

- **维度**：等于类别总数 $|V|$  
- **稀疏性**：大部分元素为 0  
- **唯一性**：每个类别对应唯一向量  
- **无序性**：向量之间没有自然顺序或距离概念

---

## 4. 优缺点

### 优点
- 简单易用，直观  
- 可直接用于 **逻辑回归、SVM、树模型** 等机器学习模型  
- 与 Embedding 结合可生成稠密向量表示

### 缺点
- 高维稀疏，浪费存储空间  
- 无法捕捉类别之间的相似性（“cat” 与 “dog” 的距离和 “cat” 与 “car” 一样）  
- 随着词汇表或类别数增加，向量维度线性增长

---

## 5. 工程实践

- **NLP**：
  - 小规模词表：可直接用 One-Hot  
  - 大规模词表：通常结合 **Embedding Table** 将稀疏向量映射为低维稠密向量  
- **分类特征**：
  - 离散特征（如性别、地区、类别标签）可使用 One-Hot  
  - 大类别数时，可考虑 **Label Encoding + Embedding** 或 **Hashing Trick**  

- **存储优化**：
  - 使用稀疏矩阵表示（`scipy.sparse`）  
  - 避免对大词表直接生成稠密矩阵  

---

## 6. 数学公式总结

假设类别集合为 $$C = \{c_1, c_2, ..., c_n\}$$，对于类别 $c_i$，One-Hot 编码：

$$
\mathbf{v}_{c_i} = [0, \dots, 0, \underbrace{1}_{i\text{-th}}, 0, \dots, 0] \in \mathbb{R}^n
$$

两类不同类别向量之间的欧式距离为：

$$
\| \mathbf{v}_{c_i} - \mathbf{v}_{c_j} \|_2 = \sqrt{2}, \quad i \ne j
$$

表明 **所有类别等距，无语义信息**。

---

## 7. 总结

- **One-Hot 编码**是离散特征向量化的基础方法  
- 高维稀疏，简单易用，但缺乏语义信息  
- 工程上常作为 Embedding 的输入，或用于小规模离散特征的机器学习模型  
- 与稠密向量方法结合可兼顾可解释性和低维高效性  

---
