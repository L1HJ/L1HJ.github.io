## 1. 引言
在自然语言处理和信息检索的早期阶段，**TF-IDF** 是最经典的特征表示方法之一。  
它的思想是：  
- 如果某个词在一篇文档中频繁出现（高 TF），它可能代表该文档的主题；  
- 如果某个词在所有文档中都很常见（低 IDF），它对区分文档没有帮助。  

因此，**TF-IDF** 衡量了一个词在文档中的重要性。  
它至今仍被广泛应用在 **搜索引擎、关键词提取、稀疏检索** 等场景中。

---

## 2. TF-IDF 公式

给定一个词 $t$、文档 $d$ 和语料库 $D$，TF-IDF 定义为：

$$
\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
$$

### 2.1 词频 (Term Frequency, TF)
$$
\text{TF}(t, d) = \frac{f(t, d)}{|d|}
$$

其中：
- $f(t, d)$ = 词 $t$ 在文档 $d$ 中出现的次数  
- $|d|$ = 文档 $d$ 的总词数  

### 2.2 逆文档频率 (Inverse Document Frequency, IDF)
$$
\text{IDF}(t, D) = \log \frac{N}{n_t + 1}
$$

其中：
- $N$ = 语料库中文档总数  
- $n_t$ = 包含词 $t$ 的文档数  
- 加 $1$ 防止分母为零  

---

## 3. 直观理解
- **高 TF，高 IDF** → 词对当前文档重要（例如“量子力学”在物理论文中）  
- **高 TF，低 IDF** → 常见词，对区分作用小（例如“的”、“and”）  
- **低 TF，高 IDF** → 稀有词，但在某文档中出现一次就可能很重要  

---

## 4. 工程特点

### 优点
- 简单高效，易于实现  
- 可解释性强（关键词权重直观）  
- 在 **稀疏特征** 模型（如线性回归、SVM、逻辑回归）中效果好  

### 缺点
- 忽略词序、语义关系（“good” vs “excellent” 完全不同）  
- 高维稀疏（词表可能百万级）  
- 对长文本和同义词处理不佳  

---

## 5. 应用场景
- **搜索引擎**：文档检索（Lucene, ElasticSearch）  
- **关键词提取**：提取文档中最重要的词  
- **文本聚类/分类**：作为稀疏向量输入到传统机器学习模型  
- **信息过滤**：去除低权重的噪声词  

---

## 6. TF-IDF 与 BM25 的关系
- **TF-IDF**：词频线性增长，长度归一化较弱  
- **BM25**：引入了 TF 饱和效应 + 文档长度归一化  
- **实际效果**：BM25 通常优于 TF-IDF，但 TF-IDF 更简单，仍是一个很好的基线  

---

## 7. 总结
- **TF-IDF 是传统 NLP 的基石**，衡量词在文档中的重要性  
- 数学上简单，计算高效  
- 在现代检索系统中，常作为 **稀疏特征方法**，与 **稠密向量（Embeddings）** 结合形成 Hybrid Search  

---
