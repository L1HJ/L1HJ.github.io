
  

低温容易死循环。如果你经常debug大模型的话，死循环通常是代码写错的信号之一（通常是mask写错了）。

在代码没错的情况下，死循环意味着大模型从某个节点开始，注意力模式变成 开头一大串+最近几个token，中间的token被忽略，因此大模型每次看到的前缀是一样的，所以生成也是一样的。比如说：
  
```
我今天吃早饭了早饭了早饭了
```

这样一个生成模式中，在第一个”了“这里，模型不知由于什么原因，忽略了“早饭了”这几个字，等价于看到了”我今天吃“，然后继续补全。所以划分出来是这样的：

”我今天吃 |早饭 |了| 早饭 |了| “  
  
蹭点烂梗的话可以叫他 “我可以在忘记早饭的情况下吃完早饭吗？”（显然不能）

在代码正确的情况下，这个问题经常出现在**长文本生成尾端**，此时由于**RoPE强度衰减到几乎不能区分位置+token数实在太多，softmax对于99%以上的token赋予的概率值都是0**，将可能出现刚生成完就忘了的情况（很多时候是因为抢不过开头几个token的注意力权重）。这个现象据我所知，QWQ早期非常严重，几乎是一遇到难题就开始死循环（）。

因此对于这个问题，最可能的方法是放一条超长Prompt，并且开头信息量大，后端几乎都是废话，从而使得LLM对尾端token注意力极小，使其“说了就忘“，从而死循环。